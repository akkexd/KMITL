# -*- coding: utf-8 -*-
"""supervised_semi_supervised.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rqN6JRuw3WFo3q1938Sc7kAYSkLUPYtz
"""

import numpy as np
import matplotlib.pyplot as plt
import copy
import math
import os
import pandas as pd

def load_data(filename):
    # Use pandas to read the CSV and handle invalid entries
    data = pd.read_csv(filename, header=None)
    # Replace invalid entries with NaN
    data = data.apply(pd.to_numeric, errors='coerce')
    # Drop rows with NaN values
    data = data.dropna()

    X = data.iloc[:, :-1].values
    y = data.iloc[:, -1].values
    return X, y

def plot_decision_boundary(w, b, X, y):

    plot_data(X[:, 0:2], y)

    if X.shape[1] <= 2:
        plot_x = np.array([min(X[:, 0]), max(X[:, 0])])
        plot_y = (-1. / w[1]) * (w[0] * plot_x + b)

        plt.plot(plot_x, plot_y, c="b")

    else:
        u = np.linspace(-1, 1.5, 50)
        v = np.linspace(-1, 1.5, 50)

        z = np.zeros((len(u), len(v)))

        for i in range(len(u)):
            for j in range(len(v)):
                z[i,j] = sigmoid(np.dot(map_feature(u[i], v[j]), w) + b)

        z = z.T

        plt.contour(u,v,z, levels = [0.5], colors="g")

def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):

    m = len(X)

    J_history = []
    w_history = []

    for i in range(num_iters):

        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)

        w_in = w_in - alpha * dj_dw
        b_in = b_in - alpha * dj_db

        if i<100000:      # prevent resource exhaustion
            cost =  cost_function(X, y, w_in, b_in, lambda_)
            J_history.append(cost)

        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):
            w_history.append(w_in)
            print(f"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   ")

    return w_in, b_in, J_history, w_history

def sigmoid(z):

    g = 1/(1+np.exp(-z))

    return g

from google.colab import drive
drive.mount('/content/drive')

def compute_cost(X, y, w, b, *argv):

    m, n = X.shape

    cost = 0
    for i in range(m):
        z = np.dot(X[i],w) + b
        f_wb = sigmoid(z)
        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)
    total_cost = cost/m

    return total_cost

def plot_data(X, y, pos_label="Positive", neg_label="Negative"):

    pos = y == 1
    neg = y == 0


    plt.scatter(X[pos, 0], X[pos, 1], c='b', marker='o', label=pos_label)

    plt.scatter(X[neg, 0], X[neg, 1], c='r', marker='x', label=neg_label)

def predict(X, w, b):

    m, n = X.shape
    p = np.zeros(m)

    for i in range(m):
        z_wb = np.dot(X[i], w) + b

        f_wb = sigmoid(z_wb)

        p[i] = 1 if f_wb >= 0.5 else 0

    return p

def map_feature(X1, X2):

    X1 = np.atleast_1d(X1)
    X2 = np.atleast_1d(X2)
    degree = 6
    out = []
    for i in range(1, degree+1):
        for j in range(i + 1):
            out.append((X1**(i-j) * (X2**j)))
    return np.stack(out, axis=1)

def compute_cost_reg(X, y, w, b, lambda_ = 1):

    m, n = X.shape

    cost_without_reg = compute_cost(X, y, w, b)

    reg_cost = 0.

    for j in range(n):
        reg_cost += w[j] ** 2
    reg_cost = (lambda_ / (2 * m)) * reg_cost

    total_cost = cost_without_reg + reg_cost

    return total_cost

def compute_gradient(X, y, w, b, *argv):
    m = len(X)
    z_wb = np.dot(X, w) + b
    f_wb = sigmoid(z_wb)
    error = f_wb - y
    dj_dw = np.dot(X.T, error) / m
    dj_db = np.sum(error) / m
    return dj_db, dj_dw

def compute_gradient_reg(X, y, w, b, lambda_ = 1):

    m, n = X.shape

    dj_db, dj_dw = compute_gradient(X, y, w, b)

    for j in range(n):
        dj_dw_j_reg = (lambda_ / m) * w[j]

        dj_dw[j] += dj_dw_j_reg

    return dj_db, dj_dw

X_train, y_train = load_data("/content/ex2data1.csv")

print("Original shape of labeled data:", X_train.shape)
print("Original shape of labeled data:", y_train.shape)

plot_data(X_train, y_train, pos_label="Admitted", neg_label="Not admitted")


plt.ylabel('Exam 2 score')

plt.xlabel('Exam 1 score')
plt.legend(loc="upper right")
plt.show()

np.random.seed(1)
initial_w = 0.01 * (np.random.rand(2) - 0.5)
initial_b = -8

iterations = 10000
alpha = 0.001

w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b,compute_cost, compute_gradient, alpha, iterations, 0)

plot_decision_boundary(w, b, X_train, y_train)

plt.ylabel('Exam 2 score')
plt.xlabel('Exam 1 score')
plt.legend(loc="upper right")
plt.show()

p = predict(X_train, w,b)
print('Train Accuracy: %f'%(np.mean(p == y_train) * 100))

def self_training(X_labeled, y_labeled, X_unlabeled, initial_w, initial_b, alpha, iterations, lambda_, confidence_threshold=0.9, max_iterations=10):
    w, b = initial_w, initial_b
    for _ in range(max_iterations):
        # Train the model on the labeled data
        w, b, _, _ = gradient_descent(X_labeled, y_labeled, w, b, compute_cost, compute_gradient, alpha, iterations, lambda_)
        # Predict probabilities on the unlabeled data
        y_probs = sigmoid(np.dot(X_unlabeled, w) + b)
        confidences = np.abs(y_probs - 0.5) * 2
        confident_indices = np.where(confidences >= confidence_threshold)[0]
        if len(confident_indices) == 0:
            break
        y_preds = (y_probs >= 0.5).astype(int)
        # Add confident predictions to the labeled dataset
        X_labeled = np.vstack((X_labeled, X_unlabeled[confident_indices]))
        y_labeled = np.hstack((y_labeled, y_preds[confident_indices]))
        # Remove newly labeled data from the unlabeled set
        X_unlabeled = np.delete(X_unlabeled, confident_indices, axis=0)
    return w, b

X_train_part, y_train_part = load_data("/content/test_data.csv")
X_labeled, X_unlabeled = X_train_part[:int(0.5*len(X_train_part))], X_train_part[int(0.5*len(X_train_part)):]
y_labeled, y_unlabeled = y_train_part[:int(0.5*len(y_train_part))], y_train_part[int(0.5*len(y_train_part)):]

print("Original shape of labeled data:", X_labeled.shape)
print("Original shape of unlabeled data:", X_unlabeled.shape)

plot_data(X_labeled, y_labeled, pos_label="Admitted", neg_label="Not admitted")
plt.ylabel('Exam 2 score')
plt.xlabel('Exam 1 score')
plt.legend(loc="upper right")
plt.title("Partially Labeled Data Before Self-Training")
plt.show()

lambda_ = 0
w, b = self_training(X_labeled, y_labeled, X_unlabeled, initial_w, initial_b, alpha, iterations, lambda_)

print("Shape of labeled data after self-training:", X_labeled.shape)
print("Shape of unlabeled data after self-training:", X_unlabeled.shape)

plot_decision_boundary(w, b, X_train_part, y_train_part)
plt.ylabel('Exam 2 score')
plt.xlabel('Exam 1 score')
plt.legend(loc="upper right")
plt.title("Decision Boundary After Self-Training")
plt.show()

p = predict(X_train_part, w, b)
print('Final Train Accuracy: %f' % (np.mean(p == y_train_part) * 100))